---
title: "Sampling with Multiple Experts"
author: "Rob Schick, PhD & Michail Papathomas, PhD"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Sampling with Multiple Experts}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE, fig.width = 8, fig.height = 6)
```


```{r setup, echo=FALSE}
rm(list = ls())
library(eliciteg)
```

# Goal of the Vignette
Here we show how to sample from the Mixture Dirichlet posterior. This is done to sample from the multiple experts' prior beliefs about movement of right whales in and around the mid-Atlantic region. The protocol for sampling is as follows:

1. Assemble movement data
2. Assemble priors from multiple individual experts
3. Calculate normalizing constant $C$ 
4. Error check the $C$ matrix for numerical stability
5. Exponentiate the $C$ matrix
6. Use the data and $C$ to calculate the $K$ priors
7. Sample from the Dirichlet to simulate the $K$ posteriors

# The Data
In the model from Schick et al. (2013), monthly movement probabilities are sampled from a Dirichlet posterior. Informed priors come from one expertâ€”Philip Hamilton, New England Aquarium. The sightings data come from the North Atlantic Right Whale Consortium; missing data are imputed with multiple imputation. Because we are using mulitple imputation, we need to sample to approximate the posterior.

In this vignette, we will work through a toy example using simulated data first, and then proceed to use real data. Here is a toy dataset, representing movements of whales in 5 regions. 

```{r makeToyData}
maleExT <- matrix(c(1, 0, 2, 1, 1,
                      6, 2, 0, 1, 3,
                      3, 0, 0, 3, 1,
                      9, 0, 13, 5, 2,
                      1, 0, 0, 0, 3),
                    nrow = 5, ncol = 5, byrow = TRUE)
colnames(maleExT) <- c("BOF", "GOM", "GSC", "JL", "MIDA")
```

For this toy example, we'll assume two experts. Here are their priors:

```{r makeToyPriors}
priorT_1 <- matrix(c(2.502, 0.39485714, 0.39485714, 0.39485714, 5.952,
                       0.1448571, 0.04791837, 0.04791837, 0.04791837, 0.003,
                       0.1448571, 0.04791837, 0.04791837, 0.04791837, 0.003,
                       0.1448571, 0.04791837, 0.04791837, 0.04791837, 0.003,
                       1.502, 0.003, 0.003, 0.003, 1.052),
                     nrow = 5, ncol = 5, byrow = TRUE)

priorT_2 <- matrix(c(3.502, 2.39485714, 1.39485714, 1.39485714, 3.952,
                       0.1448571, 1.04791837, 1.04791837, 1.04791837, 0.003,
                       1.1448571, 0.04791837, 1.04791837, 2.04791837, 0.003,
                       1.1448571, 0.04791837, 1.04791837, 1.04791837, 0.003,
                       0.502, 1.003, 1.003, 1.003, 2.052),
                     nrow = 5, ncol = 5, byrow = TRUE)

priorT <- array(1, dim = c(5, 5, 2))
priorT[, , 1] <- priorT_1
priorT[, , 2] <- priorT_2
nreg <- ncol(priorT_1)
```

With those in place, now we proceed to calculate the $C$ normalizing constants. First, we set up a placeholder matrix:

```{r makeCmat}
cmatOut <- matrix(NA, nrow = dim(priorT)[3], 
                  ncol = nreg,
                  dimnames = list(c('expert1', 'expert2'),
                                  colnames(maleExT)))
```

Note that the rows in `cmatOut` correspond to the number of experts, while the columns correspond to the number of regions. With that set up, we can populate it with a nested set of loops:

```{r}
for(i in seq_along(1:dim(maleExT)[2])){
  for(j in seq_along(1:dim(priorT)[3])){
    prior <- priorT[, , j]
    cmatOut[j, i] <- calcC(maleExT[, i], prior[, i])
  }
}
cmatOut
```

This looks fine, and the next step would be to simply exponentiate this $C$ matrix. However, in our exploratory work to date with this dataset, we've run into situtations where the values in `cmatOut` get very big and negative. In this case taking the exponent, yields 0 values, which we can't have in this algorithm. 

The solution Michail came up with is to add some numerical padding to all experts' values in the instances where one expert has a very negative value. This preserves the relationships among experts, while keeping things tractable numerically. Here's the solution in the form of a new function:

```{r}
transformC
```

In use:

```{r}
cmatOut <- transformC(cmatOut, tval = -600)
cmatOut
```

Though note that in this case, no values in the original `cmatOut` were below -600, so all we get back is `exp(cmatOut)`. However, as we'll see with the real data later, this will come into play.

# Assemble $K$
With the $C$ matrix assembled, we can now assemble the $K$ weights. 

Mathematically, the overall posterior is represented as:

$$ f^{(1)} (\pi) = \sum^J_{j=1} k_j^{(1)} f^{(1)}_j (\pi),$$

with 

$$k_j^{(1)} = \frac{k_j^{(0)} C_j}{\sum_{i=1}^J k_j^{(0)} C_j} .$$

To sample, you choose a component distribution $j$with probability $k_j^{(0)}$. We then sample from $f_j^{(1)} (\pi) = Dir \left( d^{(0)}_{j1} + n_1, \ldots, d^{(0)}_{jM} + n_M\right )$, assuming our data are $(n_1, \ldots, n_M)$.

Our function to assemble this matrix is:

```{r}
calcK
```

In use:

```{r}
kmat <- calcK(cmatOut)
kmat
colSums(kmat)
```

That output, then, is a matrix where each column is the probability vector (that sums to 1), which we will use to select the expert within the Gibbs loop.

# Sampling From the Dirichlet
With the toy example, we'll sample directly from a Dirichlet making use of the `rdirichlet()` function in the `gtools` library. First we can view the prior for each expert:

## Priors
Let's see how our experts stack up for the prior for the first region: Bay of Fundy (BOF). First we sample them

```{r}
i <- 1 # the region where we assume a whale is present
library(gtools)
pr1dirsample <- pr2dirsample <- matrix(NA, nrow = 1000, ncol = nreg + 1)
for (ig in 1:1000) {
    
  pr1dirsample[ig, 1:nreg] <- rdirichlet(1, priorT[, i, 1]) 
  pr2dirsample[ig, 1:nreg] <- rdirichlet(1, priorT[, i, 2])
  
}
pr1dirsample[, (nreg + 1)] <- 1
pr2dirsample[, (nreg + 1)] <- 2
prdirsample <- rbind(pr1dirsample, pr2dirsample)
colnames(prdirsample) <- c(colnames(kmat), 'expert')
```

And then we can plot those:
```{r fig.height=4}
library(ggplot2)
library(reshape2)
xl <- melt(as.data.frame(prdirsample), id = 6)

# draw the histograms
ggplot(data = xl, aes(x = value, colour = factor(expert)))+
  geom_density()+
  facet_wrap(~variable, nrow = 1)+
  xlim(0, 1)

```

## Posteriors
And then we can move on to the posterior
```{r}
postdirsample <- rdirichlet(1000, rep(1, nreg)) # placeholder for sampled values
for (ig in 1:1000) {
  
  idx <- which(rmultinom(1, 1, kmat[, i]) == 1)
  prior <- priorT[, i, idx]
  postdirsample[ig, ] <- rdirichlet(1, maleExT[, i] + prior)
  
}
```

And then we can plot those:
```{r fig.height=4}
colnames(postdirsample) <- colnames(kmat)
xl <- melt(postdirsample)

# draw the histograms
ggplot(data = xl, aes(x = value, group = Var2))+
  geom_histogram(fill = 'cornsilk', colour = 'grey20', size = 0.2)+
  facet_wrap(~Var2, nrow = 1)+
  xlim(0, 1)

```

# Real Data
Ok, that seems to work with a toy data example for one region. Let's try it with a full 9 * 9 data matrix. Again, we won't use multiple imputation here, though we will in the paper. This is just to see it in action. Note that where above we sampled directly from the Dirichlet, here we'll use a slightly different algorithm. 

