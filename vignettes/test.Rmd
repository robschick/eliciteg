---
title: "Merging Multiple Dirichelt Distributions"
author: "Rob Schick, PhD and Michail Papathomas, PhD"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Merging Multiple Dirichelt Distributions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Herein we describe the second of the two steps needed to go from the (multiple) raw values we elicited as Beta distributions from the expert into a coherent probability framework. What we describe here is how to take the individual Dirichlet distributions we created in the first vignette, and create a merged Dirichlet. As before we will outline the mathematics, use a toy example, and then work through a proper example with code.

## Mathematical Background
Let's assume we have a prior with $m$ categories that is a mixture of Dirichlet's:

$$f^0_{(n)} = \sum_{j = 1}^J k_j^{(0)} f_j^{(0)} (n)$$

$$f^0_{(n)} = \sum_{j = 1}^J k_j^{(0)} Dir\left( d^{(0)}_{j1}, \ldots, d^{(0)}_{jm} \right)$$

Note the non-standard notation where the $^{(0)}$ superscript refers to the prior, and later $^{(1)}$ will refer to the posterior.

To sample from the above distribution, we choose a component distribution $j$ with probability $k_j^{(0)}$. We then sample from $f_j^{(0)} (n)$, assuming our data are $(n_1, \ldots, n_m)$. The posterior for each component is given as:

$$ f_j^{(0)} (n) = Dir \left( d^{(0)}_{j1} + n_1, \ldots, d^{(0)}_{jm} + n_m\right ),$$

with normalising constant $C$ given as:

$$ C_j = \frac{\prod^M_{m=1}\Gamma(d_{jm}^{(0)} + n_m)}{\Gamma \left( \sum^M_{m = 1} (d_{jm}^{(0)} + n_m)\right)}.$$

The overall posterior is:

$$ f^{(1)} (n) = \sum^J_{j=1} k_j^{(1)} f^{(1)}_j (n),$$

with 

$$k_j^{(1)} = \frac{k_j^{(0)} C_j}{\sum_{j=1}^J k_j^{(0)} C_j} .$$

To sample, you choose a component distribution $j$with probability $k_j^{(0)}$. We then sample from $f_j^{(1)} (n) = Dir \left( d^{(0)}_{j1} + n_1, \ldots, d^{(0)}_{jm} + n_m\right )$, assuming our data are $(n_1, \ldots, n_m)$.

## Posterior Calculations
Ok, let's work through a toy example with fake data. First, let's assume we have data with $M$ categories $(15, 25, 60, \ldots, 21)$ and one Dirichlet prior distribution:

$$f_1^{(0)} = Dir(d_{11}^{(0)}, d_{12}^{(0)}, \ldots, d_{1M}^{(0)})$$

The posterior from which we'd sample is:

$$ f_1^{(1)} = Dir(d_{11}^{(0)} + 15, d_{12}^{(0)} + 25, d_{13}^{(0)} + 60, \ldots, d_{1M}^{(0)} + 21).$$

This is easy. But what if we have multiple Dirichlet priors from 3 different experts? Now we have three Dirichlet distributions $D_1, D_2, D_3$ with the data as enumerated above. The prior is 

$$Prior\: = \: \frac{1}{3}f_1^{(0)} +  \frac{1}{3}f_2^{(0)} + \frac{1}{3}f_3^{(0)} $$

The posterior is a mixture of these Dirichlet's, but NOT with $\frac{1}{3}$ weights. Now we explain the proper weighting.

Say the prior weights can be written as:

$$ K^{(0)}_1 = \frac{1}{3}, K^{(0)}_2 = \frac{1}{3}, K^{(0)}_3 = \frac{1}{3} $$

and that we have three posterior Dirichlet's: 

$$ f_1^{(1)} = Dir(d_{11}^{(0)} + 15, d_{12}^{(0)} + 25, d_{13}^{(0)} + 60, \ldots, d_{1M}^{(0)} + 21) $$

$$ f_2^{(1)} = Dir(d_{21}^{(0)} + 15, d_{22}^{(0)} + 25, d_{23}^{(0)} + 60, \ldots, d_{2M}^{(0)} + 21) $$

$$ f_3^{(1)} = Dir(d_{31}^{(0)} + 15, d_{32}^{(0)} + 25, d_{33}^{(0)} + 60, \ldots, d_{3M}^{(0)} + 21) $$

Our new posterior is:  

$$ K^{(1)}_1 f_1^{(1)} + K^{(1)}_2 f_2^{(1)} + K^{(1)}_3 f_3^{(1)}. $$

Next we calculate the weights. To do this we first calculate the normalising constant $C$ for each prior, and then calculate the full prior weight for each of the three distributions.

$$ C_1 = \frac{\prod^M_{m=1}\Gamma(d_{1m}^{(0)} + n_m)}{\Gamma \left( \sum^M_{m = 1} (d_{1m}^{(0)} + n_m)\right)}, $$

$$ C_2 = \frac{\prod^M_{m=1}\Gamma(d_{2m}^{(0)} + n_m)}{\Gamma \left( \sum^M_{m = 1} (d_{2m}^{(0)} + n_m)\right)}, $$

$$ C_3 = \frac{\prod^M_{m=1}\Gamma(d_{3m}^{(0)} + n_m)}{\Gamma \left( \sum^M_{m = 1} (d_{3m}^{(0)} + n_m)\right)}. $$

All three of these are then included in the complete calculation for $K^{(1)}_j$

$$ K^{(1)}_1  = \frac{K^{(0)}_1 C_1}{K^{(0)}_1 C_1 + K^{(0)}_2 C_2 + K^{(0)}_3 C_3} = \frac{\frac{1}{3} C_1}{\frac{1}{3} C_1 + \frac{1}{3} C_2 + \frac{1}{3} C_3},$$

$$ K^{(1)}_2  = \frac{K^{(0)}_2 C_2}{K^{(0)}_1 C_1 + K^{(0)}_2 C_2 + K^{(0)}_3 C_3} = \frac{\frac{1}{3} C_2}{\frac{1}{3} C_1 + \frac{1}{3} C_2 + \frac{1}{3} C_3},$$

$$ K^{(1)}_3  = \frac{K^{(0)}_3 C_3}{K^{(0)}_1 C_1 + K^{(0)}_2 C_2 + K^{(0)}_3 C_3} = \frac{\frac{1}{3} C_3}{\frac{1}{3} C_1 + \frac{1}{3} C_2 + \frac{1}{3} C_3}.$$

The posterior is:  

$$ K^{(1)}_1 f_1^{(1)} + K^{(1)}_2 f_2^{(1)} + K^{(1)}_3 f_3^{(1)}. $$

To sample from this, we first sample 1, 2 or 3 with probability $K$. Let's say we choose 2, then we sample from:

$$ f_2^{(1)} = Dir(d_{21}^{(0)} + 15, d_{22}^{(0)} + 25, d_{23}^{(0)} + 60, \ldots, d_{2M}^{(0)} + 21) $$

We repeat this many times, and then build up the mixture distribution.

## Worked Example
Here we'll use the same structure as above with the same number of Dirichlets (3), the same data, and the same $m$ categories. The real trial will include an expanded number of experts, but we'll keep it compact for the sake of the example. Here are the data along with each of the three expert's Dirichlet's. The experts have low, medium, and high confidence, respectively. We create these $n$ by sampling from different `rgamma()` distributions:

```{r, fig.width=6, fig.height=6}
library(ggplot2)
dir1 <- floor(rgamma(9, 2, 0.5));dir1[dir1 == 0] <- 1
dir2 <- floor(rgamma(9, 9, 0.5));dir2[dir2 == 0] <- 1
dir3 <- floor(rgamma(9, 10, 2));dir3[dir3 == 0] <- 1

allDirs <- data.frame(expert = rep(1:3, each = 9), priors = c(dir1, dir2, dir3))
allDirs$priors[allDirs$priors == 0] <- 1

ggplot(allDirs, aes(x = priors, fill = factor(expert))) + 
  geom_density(alpha = 0.2)

simDat <- c(15, 25, 60, 5, 17, 23, 1, 35, 21)

df <- data.frame(data = simDat,
                 expert1 = dir1,
                 expert2 = dir2,
                 expert3 = dir3)
knitr::kable(df, caption = "Table 1. Data for worked example and 3 experts' prior Dirichlet distributions.")
```

First we'll write the function to accept the prior and the data and return the normalising constant $C_j$:

```{r, returnC}

calcC <- function(data, prior){
  dd <- as.vector(data)
  pp <- as.vector(prior)
  if(length(dd) != length(pp)) stop('Data and prior lengths do not match')
  c <- prod(lgamma(dd + pp)) / lgamma(sum(dd + pp)) 
  # note that without using log gamma, this blows up.
  return(c)
}

calcC(simDat, dir1)
```

To build that up for each expert we would have this in R code:

```{r}
c1 <- calcC(simDat, dir1)
c2 <- calcC(simDat, dir2)
c3 <- calcC(simDat, dir3)

k1 <- (1/3 * c1) / (1/3 * c1 + 1/3 * c2 + 1/3 * c3)
k2 <- (1/3 * c2) / (1/3 * c1 + 1/3 * c2 + 1/3 * c3)
k3 <- (1/3 * c3) / (1/3 * c1 + 1/3 * c2 + 1/3 * c3)
K <- c(k1, k2, k3)
```

With those assembled, we now want to sample according to probability $K$, which is:

```{r, showK}
round(K, 3)
```

You can see from the strong confidence of Expert 3, their prior will have a lot of weight on the final distribution. On to sampling for one iteration:

```{r}
a <- runif(1)
idx <- which.min(abs(K - a))
prior <- allDirs[allDirs$expert == idx, 'priors']
library(gtools)
post <- rdirichlet(1, simDat + prior)
round(post, 3)
```

We can now do this in a proper sampling framework and build up the posterior distribution.

```{r}
nsamp <- 10000
outdf <- matrix(nrow = nsamp, ncol = 9)
colnames(outdf) <- c('mida2mida', 'mida2seus', 'mida2bof', 'mida2gom', 
                     'mida2gsc', 'mida2jl', 'mida2ne', 'mida2nrth', 'mida2rb')
for(i in 1:nsamp){
  a <- runif(1)
  idx <- which.min(abs(K - a))
  prior <- allDirs[allDirs$expert == idx, 'priors']
  outdf[i, ] <- rdirichlet(1, simDat + prior)
}
```

And then finally, we can visualise it:

```{r, warning=FALSE, message=FALSE, fig.width=6, fig.height=6}
library(reshape2)
library(ggplot2)
dflong <- melt(as.data.frame(outdf))
p <- ggplot(data = dflong, aes(value, group = variable))+
  geom_histogram()+
  facet_grid(variable ~ .)
p
```



