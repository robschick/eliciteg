---
title: "Merging Multiple Dirichelt Distributions"
author: "Rob Schick, PhD and Michail Papathomas, PhD"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Merging Multiple Dirichelt Distributions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Herein we describe the second of the two steps needed to go from the (multiple) raw values we elicited as Beta distributions from the expert into a coherent probability framework. What we describe here is how to take the individual Dirichlet distributions we created in the first vignette, and create a merged Dirichlet. As before we will outline the mathematics, use a toy example, and then work through a proper example with code.

## Mathematical Background
Let's assume we have a prior with $m$ categories that is a mixture of Dirichlet's:

$$f^0_{(n)} = \sum_{j = 1}^J k_j^{(0)} f_j^{(0)} (n)$$

$$f^0_{(n)} = \sum_{j = 1}^J k_j^{(0)} Dir\left( d^{(0)}_{j1}, \ldots, d^{(0)}_{jm} \right)$$

Note the non-standard notation where the $^{(0)}$ superscript refers to the prior, and later $^{(1)}$ will refer to the posterior.

To sample from the above distribution, we choose a component distribution $j$ with probability $k_j^{(0)}$. We then sample from $f_j^{(0)} (n)$, assuming our data are $(n_1, \ldots, n_m)$. The posterior for each component is given as:

$$ f_j^{(0)} (n) = Dir \left( d^{(0)}_{j1} + n_1, \ldots, d^{(0)}_{jm} + n_m\right ),$$

with normalising constant $C$ given as:

$$ C_j = \frac{\prod^M_{m=1}\Gamma(d_{jm}^{(0)} + n_m)}{\Gamma \left( \sum^M_{m = 1} (d_{jm}^{(0)} + n_m)\right)}.$$

The overall posterior is:

$$ f^{(1)} (n) = \sum^J_{j=1} k_j^{(1)} f^{(1)}_j (n),$$

with 

$$k_j^{(1)} = \frac{k_j^{(0)} C_j}{\sum_{j=1}^J k_j^{(0)} C_j} .$$

To sample, you choose a component distribution $j$with probability $k_j^{(0)}$. We then sample from $f_j^{(1)} (n) = Dir \left( d^{(0)}_{j1} + n_1, \ldots, d^{(0)}_{jm} + n_m\right )$, assuming our data are $(n_1, \ldots, n_m)$.

## Posterior Calculations
Ok, let's work through a toy example with fake data. First, let's assume we have data with $M$ categories $(15, 25, 60, \ldots, 21)$ and one Dirichlet prior distribution:

$$f_1^{(0)} = Dir(d_{11}^{(0)}, d_{12}^{(0)}, \ldots, d_{1M}^{(0)})$$

The posterior from which we'd sample is:

$$ f_1^{(1)} = Dir(d_{11}^{(0)} + 15, d_{12}^{(0)} + 25, d_{13}^{(0)} + 60, \ldots, d_{1M}^{(0)} + 21).$$

This is easy. But what if we have multiple Dirichlet priors from 3 different experts? Now we have three Dirichlet distributions $D_1, D_2, D_3$ with the data as enumerated above. The prior is 

$$Prior\: = \: \frac{1}{3}f_1^{(0)} +  \frac{1}{3}f_2^{(0)} + \frac{1}{3}f_3^{(0)} $$

The posterior is a mixture of these Dirichlet's, but NOT with $\frac{1}{3}$ weights. Now we explain the proper weighting.

Say the prior weights can be written as:

$$ K^{(0)}_1 = \frac{1}{3}, K^{(0)}_2 = \frac{1}{3}, K^{(0)}_3 = \frac{1}{3} $$

and that we have three posterior Dirichlet's: 

$$ f_1^{(1)} = Dir(d_{11}^{(0)} + 15, d_{12}^{(0)} + 25, d_{13}^{(0)} + 60, \ldots, d_{1M}^{(0)} + 21) $$

$$ f_2^{(1)} = Dir(d_{21}^{(0)} + 15, d_{22}^{(0)} + 25, d_{23}^{(0)} + 60, \ldots, d_{2M}^{(0)} + 21) $$

$$ f_3^{(1)} = Dir(d_{31}^{(0)} + 15, d_{32}^{(0)} + 25, d_{33}^{(0)} + 60, \ldots, d_{3M}^{(0)} + 21) $$

Our new posterior is:  

$$ K^{(1)}_1 f_1^{(1)} + K^{(1)}_2 f_2^{(1)} + K^{(1)}_3 f_3^{(1)}. $$

Next we calculate the weights. To do this we first calculate the normalising constant $C$ for each prior, and then calculate the full prior weight for each of the three distributions.

$$ C_1 = \frac{\prod^M_{m=1}\Gamma(d_{1m}^{(0)} + n_m)}{\Gamma \left( \sum^M_{m = 1} (d_{1m}^{(0)} + n_m)\right)}, $$

$$ C_2 = \frac{\prod^M_{m=1}\Gamma(d_{2m}^{(0)} + n_m)}{\Gamma \left( \sum^M_{m = 1} (d_{2m}^{(0)} + n_m)\right)}, $$

$$ C_3 = \frac{\prod^M_{m=1}\Gamma(d_{3m}^{(0)} + n_m)}{\Gamma \left( \sum^M_{m = 1} (d_{3m}^{(0)} + n_m)\right)}. $$

All three of these are then included in the complete calculation for $K^{(1)}_j$

$$ K^{(1)}_1  = \frac{K^{(0)}_1 C_1}{K^{(0)}_1 C_1 + K^{(0)}_2 C_2 + K^{(0)}_3 C_3} = \frac{\frac{1}{3} C_1}{\frac{1}{3} C_1 + \frac{1}{3} C_2 + \frac{1}{3} C_3},$$

$$ K^{(1)}_2  = \frac{K^{(0)}_2 C_2}{K^{(0)}_1 C_1 + K^{(0)}_2 C_2 + K^{(0)}_3 C_3} = \frac{\frac{1}{3} C_2}{\frac{1}{3} C_1 + \frac{1}{3} C_2 + \frac{1}{3} C_3},$$

$$ K^{(1)}_3  = \frac{K^{(0)}_3 C_3}{K^{(0)}_1 C_1 + K^{(0)}_2 C_2 + K^{(0)}_3 C_3} = \frac{\frac{1}{3} C_3}{\frac{1}{3} C_1 + \frac{1}{3} C_2 + \frac{1}{3} C_3}.$$

The posterior is:  

$$ K^{(1)}_1 f_1^{(1)} + K^{(1)}_2 f_2^{(1)} + K^{(1)}_3 f_3^{(1)}. $$

To sample from this, we first sample 1, 2 or 3 with probability $K$. Let's say we choose 2, then we sample from:

$$ f_2^{(1)} = Dir(d_{21}^{(0)} + 15, d_{22}^{(0)} + 25, d_{23}^{(0)} + 60, \ldots, d_{2M}^{(0)} + 21) $$

We repeat this many times, and then build up the mixture distribution.
